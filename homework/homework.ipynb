{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfe2d7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contenido del zip: ['train_default_of_credit_card_clients.csv']\n",
      "Limpieza completada: ../files/input/train_data_clean.csv\n",
      "Contenido del zip: ['test_default_of_credit_card_clients.csv']\n",
      "Limpieza completada: ../files/input/test_data_clean.csv\n"
     ]
    }
   ],
   "source": [
    "# flake8: noqa: E501\n",
    "#\n",
    "# En este dataset se desea pronosticar el default (pago) del cliente el próximo\n",
    "# mes a partir de 23 variables explicativas.\n",
    "#\n",
    "#   LIMIT_BAL: Monto del credito otorgado. Incluye el credito individual y el\n",
    "#              credito familiar (suplementario).\n",
    "#         SEX: Genero (1=male; 2=female).\n",
    "#   EDUCATION: Educacion (0=N/A; 1=graduate school; 2=university; 3=high school; 4=others).\n",
    "#    MARRIAGE: Estado civil (0=N/A; 1=married; 2=single; 3=others).\n",
    "#         AGE: Edad (years).\n",
    "#       PAY_0: Historia de pagos pasados. Estado del pago en septiembre, 2005.\n",
    "#       PAY_2: Historia de pagos pasados. Estado del pago en agosto, 2005.\n",
    "#       PAY_3: Historia de pagos pasados. Estado del pago en julio, 2005.\n",
    "#       PAY_4: Historia de pagos pasados. Estado del pago en junio, 2005.\n",
    "#       PAY_5: Historia de pagos pasados. Estado del pago en mayo, 2005.\n",
    "#       PAY_6: Historia de pagos pasados. Estado del pago en abril, 2005.\n",
    "#   BILL_AMT1: Historia de pagos pasados. Monto a pagar en septiembre, 2005.\n",
    "#   BILL_AMT2: Historia de pagos pasados. Monto a pagar en agosto, 2005.\n",
    "#   BILL_AMT3: Historia de pagos pasados. Monto a pagar en julio, 2005.\n",
    "#   BILL_AMT4: Historia de pagos pasados. Monto a pagar en junio, 2005.\n",
    "#   BILL_AMT5: Historia de pagos pasados. Monto a pagar en mayo, 2005.\n",
    "#   BILL_AMT6: Historia de pagos pasados. Monto a pagar en abril, 2005.\n",
    "#    PAY_AMT1: Historia de pagos pasados. Monto pagado en septiembre, 2005.\n",
    "#    PAY_AMT2: Historia de pagos pasados. Monto pagado en agosto, 2005.\n",
    "#    PAY_AMT3: Historia de pagos pasados. Monto pagado en julio, 2005.\n",
    "#    PAY_AMT4: Historia de pagos pasados. Monto pagado en junio, 2005.\n",
    "#    PAY_AMT5: Historia de pagos pasados. Monto pagado en mayo, 2005.\n",
    "#    PAY_AMT6: Historia de pagos pasados. Monto pagado en abril, 2005.\n",
    "#\n",
    "# La variable \"default payment next month\" corresponde a la variable objetivo.\n",
    "#\n",
    "# El dataset ya se encuentra dividido en conjuntos de entrenamiento y prueba\n",
    "# en la carpeta \"files/input/\".\n",
    "#\n",
    "# Los pasos que debe seguir para la construcción de un modelo de\n",
    "# clasificación están descritos a continuación.\n",
    "#\n",
    "#\n",
    "# Paso 1.\n",
    "# Realice la limpieza de los datasets:\n",
    "# - Renombre la columna \"default payment next month\" a \"default\".\n",
    "# - Remueva la columna \"ID\".\n",
    "# - Elimine los registros con informacion no disponible.\n",
    "# - Para la columna EDUCATION, valores > 4 indican niveles superiores\n",
    "#   de educación, agrupe estos valores en la categoría \"others\".\n",
    "# - Renombre la columna \"default payment next month\" a \"default\"\n",
    "# - Remueva la columna \"ID\".\n",
    "\n",
    "\n",
    "import zipfile\n",
    "import pickle\n",
    "import gzip\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import balanced_accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def clean_dataset(df):\n",
    "    \"\"\"\n",
    "    Limpia el dataset para el modelo de clasificación.\n",
    "    - Renombra la columna objetivo a 'default'.\n",
    "    - Elimina la columna 'ID'.\n",
    "    - Elimina registros con valores faltantes.\n",
    "    - Elimina registros donde EDUCATION o MARRIAGE son cero.\n",
    "    - Agrupa EDUCATION > 4 como categoría 'others' (valor = 4).\n",
    "    \"\"\"\n",
    "    df_clean = df.drop(columns=[\"ID\"], errors=\"ignore\")  # Evita error si no existe 'ID'\n",
    "\n",
    "    if \"default payment next month\" in df_clean.columns:\n",
    "        df_clean = df_clean.rename(columns={\"default payment next month\": \"default\"})\n",
    "\n",
    "    df_clean = df_clean.dropna()\n",
    "\n",
    "    # Filtrado extra: EDUCATION y MARRIAGE no pueden ser cero\n",
    "    df_clean = df_clean[(df_clean[\"EDUCATION\"] != 0) & (df_clean[\"MARRIAGE\"] != 0)]\n",
    "\n",
    "    # Agrupamos niveles altos de EDUCATION\n",
    "    df_clean.loc[df_clean[\"EDUCATION\"] > 4, \"EDUCATION\"] = 4\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "def process_zip(zip_path, output_name):\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_contents = zip_ref.namelist()\n",
    "        print(f\"Contenido del zip: {zip_contents}\")\n",
    "        csv_name = zip_contents[0]  # Asumimos que hay solo un CSV\n",
    "        zip_ref.extract(csv_name, \"files/input/\")\n",
    "\n",
    "    df = pd.read_csv(os.path.join(\"files/input/\", csv_name))\n",
    "    df_clean = clean_dataset(df)\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_name), exist_ok=True)  # Crear carpeta si no existe\n",
    "    df_clean.to_csv(output_name, index=False)\n",
    "    print(f\"Limpieza completada: {output_name}\")\n",
    "\n",
    "# Ejecutar\n",
    "process_zip(\"../files/input/train_data.csv.zip\", \"../files/input/train_data_clean.csv\")\n",
    "process_zip(\"../files/input/test_data.csv.zip\", \"../files/input/test_data_clean.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "297798e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "X_train: (20953, 23)\n",
      "y_train: (20953,)\n",
      "X_test: (8979, 23)\n",
      "y_test: (8979,)\n"
     ]
    }
   ],
   "source": [
    "# Paso 2.\n",
    "# Divida los datasets en x_train, y_train, x_test, y_test.\n",
    "\n",
    "\n",
    "# Cargar datasets limpios\n",
    "train_df = pd.read_csv(\"../files/input/train_data_clean.csv\")\n",
    "test_df = pd.read_csv(\"../files/input/test_data_clean.csv\")\n",
    "\n",
    "# Separar variables predictoras (X) y objetivo (y)\n",
    "X_train = train_df.drop(columns=[\"default\"])\n",
    "y_train = train_df[\"default\"]\n",
    "\n",
    "X_test = test_df.drop(columns=[\"default\"])\n",
    "y_test = test_df[\"default\"]\n",
    "\n",
    "# Verificar shapes\n",
    "print(\"Shapes:\")\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_test:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1d0955b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 3.\n",
    "# Cree un pipeline para el modelo de clasificación. Este pipeline debe\n",
    "# contener las siguientes capas:\n",
    "# - Transforma las variables categoricas usando el método\n",
    "#   one-hot-encoding.\n",
    "# - Descompone la matriz de entrada usando PCA. El PCA usa todas las componentes.\n",
    "# - Estandariza la matriz de entrada.\n",
    "# - Selecciona las K columnas mas relevantes de la matrix de entrada.\n",
    "# - Ajusta una maquina de vectores de soporte (svm).\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Identificar columnas categóricas y numéricas\n",
    "categorical_cols = [\"SEX\", \"EDUCATION\", \"MARRIAGE\"]\n",
    "numerical_cols = [col for col in X_train.columns if col not in categorical_cols]\n",
    "\n",
    "# Pipeline para variables categóricas\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "# Pipeline para variables numéricas\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler())  # Estandarizar numéricas\n",
    "])\n",
    "\n",
    "# Combinar transformaciones\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"cat\", categorical_transformer, categorical_cols),\n",
    "    (\"num\", numerical_transformer, numerical_cols)\n",
    "])\n",
    "\n",
    "# Pipeline completo\n",
    "pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"pca\", PCA()),  # Todas las componentes\n",
    "    (\"select\", SelectKBest(score_func=f_classif)),  # Selección de K mejores variables\n",
    "    (\"svm\", SVC())  # Modelo SVM\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c76192b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n",
      "✅ Mejor combinación de parámetros: {'pca__n_components': 20, 'select__k': 12, 'svm__gamma': 0.099, 'svm__kernel': 'rbf'}\n",
      "✅ Mejor puntuación balanced accuracy: 0.6502\n"
     ]
    }
   ],
   "source": [
    "# Paso 4.\n",
    "# Optimice los hiperparametros del pipeline usando validación cruzada.\n",
    "# Use 10 splits para la validación cruzada. Use la función de precision\n",
    "# balanceada para medir la precisión del modelo.\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Definir hiperparámetros a probar\n",
    "param_grid = {\n",
    "    'pca__n_components': [20, 21],  \n",
    "    'select__k': [12],                 \n",
    "    'svm__kernel': ['rbf'],         \n",
    "    'svm__gamma': [0.099]           \n",
    "}\n",
    "\n",
    "# Crear GridSearchCV envolviendo el pipeline\n",
    "best_pipeline = GridSearchCV(\n",
    "    estimator=pipeline,         # pipeline del Paso 3\n",
    "    param_grid=param_grid,\n",
    "    cv=10,                      # validación cruzada con 10 splits\n",
    "    scoring=\"balanced_accuracy\",\n",
    "    n_jobs=-1,                  # usar todos los cores\n",
    "    verbose=1,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Entrenar GridSearchCV\n",
    "best_pipeline.fit(X_train, y_train)\n",
    "\n",
    "print(f\"✅ Mejor combinación de parámetros: {best_pipeline.best_params_}\")\n",
    "print(f\"✅ Mejor puntuación balanced accuracy: {best_pipeline.best_score_:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ffdf5dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Modelo guardado correctamente en: ../files/models/model.pkl.gz\n"
     ]
    }
   ],
   "source": [
    "# Paso 5.\n",
    "# Guarde el modelo (comprimido con gzip) como \"files/models/model.pkl.gz\".\n",
    "# Recuerde que es posible guardar el modelo comprimido usanzo la libreria gzip.\n",
    "\n",
    "import gzip\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Crear carpeta si no existe\n",
    "os.makedirs(\"../files/models\", exist_ok=True)\n",
    "\n",
    "# Ruta del modelo comprimido\n",
    "model_path = \"../files/models/model.pkl.gz\"\n",
    "\n",
    "# Guardar modelo comprimido\n",
    "with gzip.open(model_path, \"wb\") as f:\n",
    "    pickle.dump(best_pipeline, f)\n",
    "\n",
    "print(f\"✅ Modelo guardado correctamente en: {model_path}\")\n",
    "\n",
    "with gzip.open(\"../files/models/model.pkl.gz\", \"rb\") as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "\n",
    "# Usar el modelo cargado para predecir\n",
    "y_pred = loaded_model.predict(X_test)\n",
    "\n",
    "import gzip\n",
    "import pickle\n",
    "\n",
    "def load_model(path):\n",
    "    \"\"\"Carga un modelo previamente guardado.\"\"\"\n",
    "    with gzip.open(path, \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "    return model\n",
    "\n",
    "model = load_model(\"../files/models/model.pkl.gz\")\n",
    "\n",
    "# Usar para predecir\n",
    "y_pred = model.predict(X_test)\n",
    "model = load_model(\"../files/models/model.pkl.gz\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f5f65a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Métricas guardadas en: ../files/output/metrics.json\n"
     ]
    }
   ],
   "source": [
    "# Paso 6.\n",
    "# Calcule las metricas de precision, precision balanceada, recall,\n",
    "# y f1-score para los conjuntos de entrenamiento y prueba.\n",
    "# Guardelas en el archivo files/output/metrics.json. Cada fila\n",
    "# del archivo es un diccionario con las metricas de un modelo.\n",
    "# Este diccionario tiene un campo para indicar si es el conjunto\n",
    "# de entrenamiento o prueba. Por ejemplo:\n",
    "#\n",
    "# {'dataset': 'train', 'precision': 0.8, 'balanced_accuracy': 0.7, 'recall': 0.9, 'f1_score': 0.85}\n",
    "# {'dataset': 'test', 'precision': 0.7, 'balanced_accuracy': 0.6, 'recall': 0.8, 'f1_score': 0.75}\n",
    "\n",
    "from sklearn.metrics import precision_score, balanced_accuracy_score, recall_score, f1_score\n",
    "import json\n",
    "import os\n",
    "\n",
    "os.makedirs(\"../files/output\", exist_ok=True)\n",
    "metrics_path = \"../files/output/metrics.json\"\n",
    "\n",
    "# Predicciones\n",
    "y_train_pred = best_pipeline.predict(X_train)\n",
    "y_test_pred = best_pipeline.predict(X_test)\n",
    "\n",
    "# Métricas\n",
    "metrics_train = {\n",
    "    \"type\": \"metrics\",\n",
    "    \"dataset\": \"train\",\n",
    "    \"precision\": precision_score(y_train, y_train_pred),\n",
    "    \"balanced_accuracy\": balanced_accuracy_score(y_train, y_train_pred),\n",
    "    \"recall\": recall_score(y_train, y_train_pred),\n",
    "    \"f1_score\": f1_score(y_train, y_train_pred)\n",
    "}\n",
    "\n",
    "metrics_test = {\n",
    "    \"type\": \"metrics\",\n",
    "    \"dataset\": \"test\",\n",
    "    \"precision\": precision_score(y_test, y_test_pred),\n",
    "    \"balanced_accuracy\": balanced_accuracy_score(y_test, y_test_pred),\n",
    "    \"recall\": recall_score(y_test, y_test_pred),\n",
    "    \"f1_score\": f1_score(y_test, y_test_pred)\n",
    "}\n",
    "\n",
    "# Guardar métricas en formato JSON Lines\n",
    "with open(metrics_path, \"w\") as f:\n",
    "    f.write(json.dumps(metrics_train) + \"\\n\")\n",
    "    f.write(json.dumps(metrics_test) + \"\\n\")\n",
    "\n",
    "print(f\"✅ Métricas guardadas en: {metrics_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b288b352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Matrices de confusión guardadas en: ../files/output/metrics.json\n"
     ]
    }
   ],
   "source": [
    "# Paso 7.\n",
    "# Calcule las matrices de confusión para los conjuntos de entrenamiento y prueba.\n",
    "# Guardelas en el archivo files/output/metrics.json.\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm_train = confusion_matrix(y_train, y_train_pred)\n",
    "cm_test = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "cm_train_dict = {\n",
    "    \"type\": \"cm_matrix\",\n",
    "    \"dataset\": \"train\",\n",
    "    \"true_0\": {\"predicted_0\": int(cm_train[0][0]), \"predicted_1\": int(cm_train[0][1])},\n",
    "    \"true_1\": {\"predicted_0\": int(cm_train[1][0]), \"predicted_1\": int(cm_train[1][1])}\n",
    "}\n",
    "\n",
    "cm_test_dict = {\n",
    "    \"type\": \"cm_matrix\",\n",
    "    \"dataset\": \"test\",\n",
    "    \"true_0\": {\"predicted_0\": int(cm_test[0][0]), \"predicted_1\": int(cm_test[0][1])},\n",
    "    \"true_1\": {\"predicted_0\": int(cm_test[1][0]), \"predicted_1\": int(cm_test[1][1])}\n",
    "}\n",
    "\n",
    "# Guardar matrices en el mismo archivo metrics.json\n",
    "with open(metrics_path, \"a\") as f:  # append\n",
    "    f.write(json.dumps(cm_train_dict) + \"\\n\")\n",
    "    f.write(json.dumps(cm_test_dict) + \"\\n\")\n",
    "\n",
    "print(f\"✅ Matrices de confusión guardadas en: {metrics_path}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
